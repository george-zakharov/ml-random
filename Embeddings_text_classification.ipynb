{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Embeddings_text_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jt3giN2rwTo0",
        "g7-H5VeswqmQ"
      ],
      "authorship_tag": "ABX9TyOg1KSAVGyi4PuPNj+aK+zA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/george-zakharov/ml-random/blob/master/Embeddings_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T0nFpItvV3A",
        "colab_type": "text"
      },
      "source": [
        "# Load libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOgU9l-Vvjl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load TF 2.x\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU6YPkcHv6MK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb914c7c-e751-40a0-8b65-dc8b8b7c6d93"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53fEukCzvPRg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "94652e9e-82c0-438f-d79f-8c9f1129f420"
      },
      "source": [
        "from numpy import array\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt3giN2rwTo0",
        "colab_type": "text"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUGgHBH-wQet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [\n",
        "    # Positive Reviews\n",
        "\n",
        "    'This is an excellent movie',\n",
        "    'The move was fantastic I like it',\n",
        "    'You should watch it is brilliant',\n",
        "    'Exceptionally good',\n",
        "    'Wonderfully directed and executed I like it',\n",
        "    'Its a fantastic series',\n",
        "    'Never watched such a brillent movie',\n",
        "    'It is a Wonderful movie',\n",
        "\n",
        "    # Negtive Reviews\n",
        "\n",
        "    \"horrible acting\",\n",
        "    'waste of money',\n",
        "    'pathetic picture',\n",
        "    'It was very boring',\n",
        "    'I did not like the movie',\n",
        "    'The movie was horrible',\n",
        "    'I will not recommend',\n",
        "    'The acting is pathetic',\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyJz9tC5xJIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiments = array([1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ1dUzb9xUS7",
        "colab_type": "text"
      },
      "source": [
        "# Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHCukNOWxWpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_words = []\n",
        "for sent in corpus:\n",
        "    tokenize_word = word_tokenize(sent)\n",
        "    for word in tokenize_word:\n",
        "        all_words.append(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVpWogNnyHwd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "735bd858-7024-4879-9101-a9028e8cdbc9"
      },
      "source": [
        "unique_words = set(all_words)\n",
        "print(len(unique_words))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MslTwcVNyXiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_length = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iipd2G8TyTft",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f32e285a-fe70-46c2-a71d-2bedb0b75484"
      },
      "source": [
        "embedded_sentences = [one_hot(sent, vocab_length) for sent in corpus]\n",
        "print(embedded_sentences)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[48, 9, 34, 34, 3], [9, 1, 39, 38, 34, 43, 14], [28, 4, 1, 14, 9, 34], [37, 3], [20, 24, 3, 12, 34, 43, 14], [42, 7, 38, 6], [49, 30, 3, 7, 26, 3], [14, 9, 7, 4, 3], [35, 49], [34, 14, 6], [21, 44], [14, 39, 29, 6], [34, 21, 18, 43, 9, 3], [9, 3, 39, 35], [34, 41, 18, 14], [9, 49, 9, 21]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PF4Xfnzytxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's count max sent vector size\n",
        "word_count = lambda sentence: len(word_tokenize(sentence))\n",
        "longest_sentence = max(corpus, key=word_count)\n",
        "length_long_sentence = len(word_tokenize(longest_sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BV7Lwc6zznN2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "8c074a37-5c41-4f6e-9123-0d500d8643a3"
      },
      "source": [
        "padded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\n",
        "print(padded_sentences)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[48  9 34 34  3  0  0]\n",
            " [ 9  1 39 38 34 43 14]\n",
            " [28  4  1 14  9 34  0]\n",
            " [37  3  0  0  0  0  0]\n",
            " [20 24  3 12 34 43 14]\n",
            " [42  7 38  6  0  0  0]\n",
            " [49 30  3  7 26  3  0]\n",
            " [14  9  7  4  3  0  0]\n",
            " [35 49  0  0  0  0  0]\n",
            " [34 14  6  0  0  0  0]\n",
            " [21 44  0  0  0  0  0]\n",
            " [14 39 29  6  0  0  0]\n",
            " [34 21 18 43  9  3  0]\n",
            " [ 9  3 39 35  0  0  0]\n",
            " [34 41 18 14  0  0  0]\n",
            " [ 9 49  9 21  0  0  0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWOabSK5zzMK",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOc541Ioz1KF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_length, 20, input_length=length_long_sentence))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7-H5VeswqmQ",
        "colab_type": "text"
      },
      "source": [
        "# Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A48O1O7uwsXm",
        "colab_type": "text"
      },
      "source": [
        "https://stackabuse.com/python-for-nlp-word-embeddings-for-deep-learning-in-keras/"
      ]
    }
  ]
}