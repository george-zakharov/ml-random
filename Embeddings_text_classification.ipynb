{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Embeddings_text_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_T0nFpItvV3A",
        "jt3giN2rwTo0",
        "LQ1dUzb9xUS7",
        "YWOabSK5zzMK",
        "g7-H5VeswqmQ"
      ],
      "authorship_tag": "ABX9TyN9rp3kbbZSwW9iaXzhbfZ2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/george-zakharov/ml-random/blob/master/Embeddings_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T0nFpItvV3A",
        "colab_type": "text"
      },
      "source": [
        "# Load libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOgU9l-Vvjl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load TF 2.x\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU6YPkcHv6MK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb914c7c-e751-40a0-8b65-dc8b8b7c6d93"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53fEukCzvPRg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "94652e9e-82c0-438f-d79f-8c9f1129f420"
      },
      "source": [
        "from numpy import array\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt3giN2rwTo0",
        "colab_type": "text"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUGgHBH-wQet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [\n",
        "    # Positive Reviews\n",
        "\n",
        "    'This is an excellent movie',\n",
        "    'The move was fantastic I like it',\n",
        "    'You should watch it is brilliant',\n",
        "    'Exceptionally good',\n",
        "    'Wonderfully directed and executed I like it',\n",
        "    'Its a fantastic series',\n",
        "    'Never watched such a brillent movie',\n",
        "    'It is a Wonderful movie',\n",
        "\n",
        "    # Negtive Reviews\n",
        "\n",
        "    \"horrible acting\",\n",
        "    'waste of money',\n",
        "    'pathetic picture',\n",
        "    'It was very boring',\n",
        "    'I did not like the movie',\n",
        "    'The movie was horrible',\n",
        "    'I will not recommend',\n",
        "    'The acting is pathetic',\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyJz9tC5xJIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiments = array([1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ1dUzb9xUS7",
        "colab_type": "text"
      },
      "source": [
        "# Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHCukNOWxWpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_words = []\n",
        "for sent in corpus:\n",
        "    tokenize_word = word_tokenize(sent)\n",
        "    for word in tokenize_word:\n",
        "        all_words.append(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVpWogNnyHwd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "735bd858-7024-4879-9101-a9028e8cdbc9"
      },
      "source": [
        "unique_words = set(all_words)\n",
        "print(len(unique_words))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MslTwcVNyXiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_length = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iipd2G8TyTft",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f32e285a-fe70-46c2-a71d-2bedb0b75484"
      },
      "source": [
        "embedded_sentences = [one_hot(sent, vocab_length) for sent in corpus]\n",
        "print(embedded_sentences)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[48, 9, 34, 34, 3], [9, 1, 39, 38, 34, 43, 14], [28, 4, 1, 14, 9, 34], [37, 3], [20, 24, 3, 12, 34, 43, 14], [42, 7, 38, 6], [49, 30, 3, 7, 26, 3], [14, 9, 7, 4, 3], [35, 49], [34, 14, 6], [21, 44], [14, 39, 29, 6], [34, 21, 18, 43, 9, 3], [9, 3, 39, 35], [34, 41, 18, 14], [9, 49, 9, 21]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PF4Xfnzytxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's count max sent vector size\n",
        "word_count = lambda sentence: len(word_tokenize(sentence))\n",
        "longest_sentence = max(corpus, key=word_count)\n",
        "length_long_sentence = len(word_tokenize(longest_sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BV7Lwc6zznN2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "8c074a37-5c41-4f6e-9123-0d500d8643a3"
      },
      "source": [
        "padded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\n",
        "print(padded_sentences)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[48  9 34 34  3  0  0]\n",
            " [ 9  1 39 38 34 43 14]\n",
            " [28  4  1 14  9 34  0]\n",
            " [37  3  0  0  0  0  0]\n",
            " [20 24  3 12 34 43 14]\n",
            " [42  7 38  6  0  0  0]\n",
            " [49 30  3  7 26  3  0]\n",
            " [14  9  7  4  3  0  0]\n",
            " [35 49  0  0  0  0  0]\n",
            " [34 14  6  0  0  0  0]\n",
            " [21 44  0  0  0  0  0]\n",
            " [14 39 29  6  0  0  0]\n",
            " [34 21 18 43  9  3  0]\n",
            " [ 9  3 39 35  0  0  0]\n",
            " [34 41 18 14  0  0  0]\n",
            " [ 9 49  9 21  0  0  0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWOabSK5zzMK",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOc541Ioz1KF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_length, 20, input_length=length_long_sentence))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUaI0sBk2IYv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "ea6e3b39-339b-4f76-b3aa-398f24422b77"
      },
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "print(model.summary())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 7, 20)             1000      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 140)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 141       \n",
            "=================================================================\n",
            "Total params: 1,141\n",
            "Trainable params: 1,141\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA2QbGPp3NPI",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFCKCZwK3Mzu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "73a535e0-236f-4250-e10f-8ebfcda6e8a7"
      },
      "source": [
        "model.fit(padded_sentences, sentiments, epochs=100, verbose=1)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6947 - acc: 0.3750\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6916 - acc: 0.4375\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6886 - acc: 0.5000\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 953us/step - loss: 0.6855 - acc: 0.5625\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6824 - acc: 0.6250\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6794 - acc: 0.6875\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6763 - acc: 0.7500\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6733 - acc: 0.8125\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6702 - acc: 0.8750\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6671 - acc: 0.8750\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6641 - acc: 0.8750\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6610 - acc: 0.8750\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6579 - acc: 0.8750\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6548 - acc: 0.8750\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6516 - acc: 0.9375\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6485 - acc: 0.9375\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6453 - acc: 0.9375\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6421 - acc: 0.9375\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6389 - acc: 0.9375\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6357 - acc: 0.9375\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6324 - acc: 0.9375\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6291 - acc: 0.9375\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6258 - acc: 0.9375\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6224 - acc: 0.9375\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6190 - acc: 0.9375\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6156 - acc: 0.9375\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6122 - acc: 0.9375\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6087 - acc: 0.9375\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6052 - acc: 0.9375\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6016 - acc: 0.9375\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5980 - acc: 0.9375\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5944 - acc: 0.9375\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5907 - acc: 0.9375\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5870 - acc: 0.9375\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5832 - acc: 0.9375\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5794 - acc: 0.9375\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5756 - acc: 0.9375\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5717 - acc: 0.9375\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5678 - acc: 0.9375\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5638 - acc: 0.9375\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5599 - acc: 0.9375\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5558 - acc: 0.9375\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5518 - acc: 0.9375\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5477 - acc: 0.9375\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5435 - acc: 0.9375\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5394 - acc: 0.9375\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5352 - acc: 0.9375\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5309 - acc: 0.9375\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5267 - acc: 0.9375\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5224 - acc: 0.9375\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5181 - acc: 0.9375\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5137 - acc: 0.9375\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5094 - acc: 0.9375\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5050 - acc: 0.9375\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.5006 - acc: 0.9375\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4961 - acc: 0.9375\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4917 - acc: 0.9375\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.4872 - acc: 0.9375\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.4827 - acc: 0.9375\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4782 - acc: 0.9375\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4737 - acc: 0.9375\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4692 - acc: 0.9375\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4647 - acc: 0.9375\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.4601 - acc: 0.9375\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4556 - acc: 0.9375\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4510 - acc: 0.9375\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4465 - acc: 0.9375\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4419 - acc: 0.9375\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4374 - acc: 0.9375\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4328 - acc: 0.9375\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4283 - acc: 0.9375\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4238 - acc: 0.9375\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4192 - acc: 0.9375\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4147 - acc: 0.9375\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4102 - acc: 0.9375\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4057 - acc: 0.9375\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4012 - acc: 0.9375\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3968 - acc: 0.9375\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3923 - acc: 0.9375\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3879 - acc: 0.9375\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3835 - acc: 0.9375\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3791 - acc: 0.9375\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3747 - acc: 0.9375\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3703 - acc: 0.9375\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3660 - acc: 0.9375\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3617 - acc: 0.9375\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3574 - acc: 0.9375\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3531 - acc: 0.9375\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3489 - acc: 0.9375\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3447 - acc: 0.9375\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3405 - acc: 0.9375\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3363 - acc: 0.9375\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3322 - acc: 0.9375\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3281 - acc: 0.9375\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3241 - acc: 0.9375\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3200 - acc: 0.9375\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3160 - acc: 0.9375\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3121 - acc: 0.9375\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3082 - acc: 0.9375\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3043 - acc: 0.9375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f359ad0ce10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5urDJ8z3YQC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "014b37a5-ca03-4f95-ba2e-1efc872a1b69"
      },
      "source": [
        "loss, accuracy = model.evaluate(padded_sentences, sentiments, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 93.750000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7-H5VeswqmQ",
        "colab_type": "text"
      },
      "source": [
        "# Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A48O1O7uwsXm",
        "colab_type": "text"
      },
      "source": [
        "https://stackabuse.com/python-for-nlp-word-embeddings-for-deep-learning-in-keras/"
      ]
    }
  ]
}